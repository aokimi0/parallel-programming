 
计算机学院
并行程序设计实验报告
体系结构相关实验
姓名 : 齐一帆学号 : 2211154
专业 : 计算机科学与技术
2024年3月24日
目录
目录
1 实验综述	2
1.1 问题重述	2
1.1.1 cache 优化	2
1.1.2 超标量优化	2
1.2 环境配置	2
1.3 技术路线	2
2 问题一：cache 优化问题	3
2.1 算法设计	3
2.1.1 平凡算法	3
2.1.2 cache 优化算法	3
2.1.3 unroll 循环展开技术	4
2.2 实验设计	4
2.2.1 cache miss 的测定与应用	5
2.2.2 基于鲲鹏服务器的运行时间测定与比较	6
2.2.3 不同架构下运行时间的对比	6
3 问题二：超标量优化问题	7
3.1 算法设计	7
3.1.1 平凡链式算法	7
3.1.2 多链路式优化算法	7
3.1.3 递归算法	8
3.2 实验设计	8
3.2.1 IPC 的测定与比较	8
3.2.2 基于鲲鹏服务器的运行时间测定与比较	9
3.2.3 基于鲲鹏服务器的运行时间测定	9
4 总结	9

1 实验综述
1	实验综述
1.1	问题重述
1.1.1	cache 优化
任务要求给定一个 n × n 的矩阵，计算每一列与给定向量的内积。本实验采取三种方式对该任务进行实现：逐列访问元素的平凡算法，cache 优化算法和基于 cache 优化算法的循环展开（unroll）技术，并对这三种算法进行详细的比较和实验分析，得到最佳循环展开次数，评估 cache 优化和循环展开技术相较于平凡算法的性能表现。
1.1.2	超标量优化
任务要求计算 n 个数的和，本实验采取三种方式对该任务进行实现：逐个累加的平凡算法（链式）和适合超标量架构的指令级并行算法（相邻指令无依赖）包括最简单的两路链式累加算法和递归算法，通过在多平台进行实验分析算法的性能表现。
1.2	环境配置
	特性/参数	架构	CPU 最大频率 CPU 最小频率 L1 cache	L2 cache	L3 cache
ARM (鲲鹏处理器)	aarch64	2600MHz	200MHz	64K	512K	48MB
Intel (Core i7-1065G7)	x86_64	3900MHz	1300MHz	48K	512K	8MB
表 1: 环境配置参数
1.3	技术路线
 
图 1.1: 技术路线
 
2	问题一：cache 优化问题
2.1	算法设计
2.1.1	平凡算法
逐行访问元素的平凡算法的代码如下：
Listing 1: 逐列访问元素的平凡算法
void col_major_calc () {
for (int i = 0; i < N; i++) { result [ i ] = 0;
	for (int j = 0;	j < N;	j++) {
result [ i ] += matrix [ j ] [ i ] ∗ vector [ j ] ;
}
}
}
1
2
3
4
5
6
7
8
缓存行是缓存中的基本存储单位，当程序访问某个内存位置时，整个缓存行都会被加载到缓存中。
理想情况下，紧随着当前访问的数据，程序接下来访问的数据也在同一个缓存行中，这样就可以直接从缓存中快速获取数据，而无需再次访问较慢的主内存。
因此，由于平凡算法的这种跳跃式访问模式，每次访问矩阵的一个元素时，都可能导致缓存未命中，从而需要从主内存中重新加载数据，这显著增加了访问延迟，导致缓存利用率大大降低。这不仅减慢了算法的执行速度，还增加了处理器和内存系统的工作负担。
2.1.2	cache 优化算法
cache 算法代码如下：
Listing 2: cache 优化算法
void row_major_calc() {
for (int i = 0; i < N; i++) { result [ i ] = 0;
}
for (int j = 0;	j < N;	j++) { for (int i = 0;	i < N;	i++) {
result [ i ] += matrix [ j ] [ i ] ∗ vector [ j ] ;
}
}
}
1
2
3
4
5
6
7
8
9
10
上述通过考虑空间局部性原理，有效提升了性能。通过逐行访问矩阵 M，即实现了对矩阵 M 占用的内存的连续访问，这种访问模式与现代计算机存储系统中的数据存储和缓存机制高度契合。矩阵在内存中通常是按行顺序存储的，因此逐行访问可以确保每次从内存加载数据到缓存时都能最大化利用缓存行（cache line）中的数据。
2.1.3	unroll 循环展开技术
利用 unroll 循环展开技术对代码进行优化，其思想如下：
 
Algorithm 1 Matrix-Vector Multiplication with Loop Unrolling by a Factor of x
1: procedure Unroll_x(Matrix,V ector,Result,N,x)
2:	for 	do
3: 4:	end for
5:
 	fordo
remainder	N mod x
for
9: 10:		i] ×	[j]
Result[i + 1] ← Result[i + 1] + Matrix[j][i + 1] × V ector[j]
...
11: 12:	Result[i + x − 1] ← Result[i + x − 1] + Matrix[j][i + x − 1] × V ector[j] end for
13:	for 
14: 15:	[j][i] ×	[j] end for
16:	end for
17: end procedure
循环展开技术通过减少循环迭代的次数，能有效降低程序的运行时间。接下来的实验部分将探索不同循环展开次数的效果，并针对各种数据规模确定最优的展开程度。通过对循环展开次数的精确调整，提升程序的性能表现。
2.2	实验设计
为便于实验测试，设计如下测试函数：
Listing 3: 测试函数
void test (void (∗func) () , int n) {
init () ;
auto start = std : : chrono : : high_resolution_clock : : now() ; for (int i = 0; i < n; i++) {
func () ;
} auto end = std : : chrono : : high_resolution_clock : : now() ;
std : : chrono : : duration<double, std : : milli> elapsed = end − start ; std : : cout << "average time: " << elapsed . count () / n << " ms" << std : : endl ;
}
1
2
3
4
5
6
7
8
9
10
本测试函数具有如下特点：
•	测量运行时间的方法：使用 C++11 标准中的 std::chrono::high_resolution_clock 测量时间，而不使用平台特定的 API(如 QueryPerformanceCounter、clock 等)，增加了程序的可移植性。
•	提高测量精确度：将 func 函数运行 n 次，求其运行时间的平均值，使测量结果更加准确。
•	操作便捷性：利用 argc 向 main 函数中传递参数，方便多平台测试与移植。
2.2.1	cache miss 的测定与应用
Cache miss 是指处理器尝试从缓存中读取数据时未能找到对应的数据条目，因而不得不从更高级别的存储（如主内存）中读取数据的情况。在其他条件相同的情况下，cache miss 越小，程序运行速度越快。利用 cache miss 我们可以完成两项工作：
•	测定平凡算法（列访问模式）与 cache 优化算法（行访问模式）在不同数据规模下的 cache miss 值，以反映其优化效果。
•	测定不同循环展开次数的 cache miss 值，以选择在不同数据规模下的循环展开次数。
利用鲲鹏服务器环境，实验测得不同数据规模下的 cache miss 值如下图所示：
 
图 2.2: 不同情况下的 cache miss 值
左图是平凡算法（col）、cache 优化算法（row）和循环展开 5、10、15、20 次时 cache miss 的值；右图是四种循环展开次数对应的 cache miss，并标注出在不同的数据规模下，哪种循环展开次数最好。
由上图我们可以获得如下结论：
•	由左图可得，cache 优化后的 cache miss 值远远小于平凡算法，是平凡算法的十分之一，证明优化后性能有极大提升。
•	由左图可得，利用循环展开技术继续优化 cache 算法，无论展开多少个循环，cache miss 的值均小于优化前的值（row），证明了循环展开技术对于程序性能提升的有效性。
•	由右图可得，针对不同数据规模，性能最好的循环展开次数不同。随着数据规模的增大，循环展开次数越多，程序的 cache miss 越少，运行时间越短，性能越好。
根据上述结论，证明了 cache 优化的有效性，并为接下来的实验做了准备，即确定了接下来的实验中，不同数据规模下所用的循环展开次数：
数据规模	1000	2000	3000	4000	5000	6000	7000	8000	9000	10000
展开次数	10	10	15	20	20	20	20	20	20	20
表 2: 循环展开次数确定
2.2.2	基于鲲鹏服务器的运行时间测定与比较
在鲲鹏服务器上，实验测定了矩阵规模在 100-1000、1000-10000 时不同算法的运行时间，左图为矩阵规模在 100-1000 的变化，右图为矩阵规模为 1000-10000 的变化，测定结果如下：
 
图 2.3: 运行时间随矩阵规模的变化
从实验中，我们可以得到如下结论：
•	无论矩阵规模多大，cache 优化算法的运行时间都比平凡算法低。
•	在矩阵规模小于 3000 时，cache 优化算法的运行时间与平凡算法运行时间差距不大，在矩阵规模大于 3000 二者差异会逐渐变大。
•	利用循环展开技术的运行时间相较于 cache 优化算法的运行时间进一步降低。
上述实验结果符合事实情况：CPU 的 L3 缓存大小为 48MB，对于每个 unsigned long long int 类型的数组元素占用 10 字节的情况下，完全填充各级缓存需要的元素数量 2000。当数组大小超过 3000，逐列访问的方法在 L3 缓存的命中率也大幅下降，迫使其频繁从系统内存中读取数据，导致显著增加的内存访问开销，从而在逐行与逐列访问方法之间产生了明显的性能差距。
2.2.3	不同架构下运行时间的对比
在 x86 的架构下，对不同算法的运行时间进行测定，其中设置循环展开次数为 15：
 
3 问题二：超标量优化问题
 
图 2.4: x86 架构下运行时间的测定
cache 优化算法在 x86 的架构下同样有效，但在实验过程中我们发现，利用 x86 的运行时间不够稳定，这可能由于在多任务操作系统中，多个进程和线程可能会竞争 CPU 时间、内存、I/O 等资源。
这种竞争可以导致运行时间的波动。相比之下，如果鲲鹏服务器被配置为在更控制的环境中运行特定应用，可能会减少这种竞争，从而提高性能的稳定性。
3	问题二：超标量优化问题
3.1	算法设计
3.1.1	平凡链式算法
Listing 4: 平凡链式算法
}	void naive_sum(int N) { S = 0; for (int i = 0; i < N; i++) { S += V[ i ] ;
}
1
2
3
4
5
6
平凡的链式算法中，每条加法指令都依赖于前一条指令的结果，没有进行流水线优化，CPU 无法并行执行多条加法指令，无法充分利用超标量。
3.1.2	多链路式优化算法
在多链路优化算法中，我们考虑两种情况：二路优化和四路优化。以较为复杂的四路优化为例，其代码如下：
Listing 5: 多链路式优化算法
void multiplexing4_sum(int N) { int64_t s1 = 0;
1
2
3 问题二：超标量优化问题
}	int64_t s2 = 0; int64_t s3 = 0; int64_t s4 = 0; S = 0; for (int i = 0; i < N − 3; i += 4) { s1 += V[ i ] ; s2 += V[ i + 1]; s3 += V[ i + 2]; s4 += V[ i + 3];
}
for (int i = N − N % 4; i < N; ++i ) { s1 += V[ i ] ;
}
S = s1 + s2 + s3 + s4 ;
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
多链路加法策略针对超标量流水线架构进行了优化，有效消除了相邻加法操作之间的依赖关系。这一改进使得 CPU 能够将多个加法操作并行分配至不同的流水线上执行，从而提高处理效率和计算速度。
3.1.3	递归算法
Listing 6: 递归算法
void recursive_sum(int N) { std : : vector<int64_t> tempV = V;
for (int m = N; m > 1; m /= 2) { for (int i = 0; i < m / 2; i++) { tempV[ i ] = tempV[2 ∗ i ] + tempV[2 ∗ i + 1];
}
}
S = tempV[ 0 ] ;
}
1
2
3
4
5
6
7
8
9
递归加法同样考虑到了超标量流水线优化问题，消除了临近加法指令之间的依赖。进一步地，实验将递归算法改写为循环的形式，来消除递归调用的开销。
3.2	实验设计
3.2.1	IPC 的测定与比较
基于 ARM 和 X86 两种架构，本实验对 IPC 进行了测定，IPC 越高，反映了该算法的性能与运行效率更佳，并行程度更好。测定结果如下：
架构 平凡算法 二路优化 四路优化 递归算法 ARM	1.23	1.49	1.58	1.74 X86	0.98	1.42	1.48	1.70
 
表 3: 循环展开次数确定
4 总结
3.2.2	基于鲲鹏服务器的运行时间测定与比较
3.2.3	基于鲲鹏服务器的运行时间测定
实验对比测定了不同数据规模下平凡算法（ori），二路并行优化算法（mul2），四路并行优化算法
（mul4）和递归算法（rec）的运行时间。由于将所有数据绘制在同一张图中在数据规模小于 22̂1 次方时无法只管看出其差距，故分段绘制图像如下：
 
图 3.5: 不同算法运行时间对比
由上图我们可以得出如下结论：
•	在不同的数据规模之下，算法运行时间变化的趋势均相同。
•	递归算法的运行时间远大于其他算法，进行链路优化的算法运行时间小于未优化的平凡算法，在两种链路优化算法中，四路优化的效果比二路优化更佳。
综合 CPI 与运行时间的测定结果，我们可以看到四路优化算法的并行程度大于二路优化，其算法性能也更佳，运行时间更短，这证明并行度较高的程序往往具有更好地性能，因为 CPU 可以同时处理多条指令。
然而，我们可以发现一个有趣的现象：递归算法的并行程度最高，如果仅考虑并行程度这一个变量，那么递归算法理应运行时间最短，然而事实却截然相反，递归算法的运行时间远远长于其他算法，其原因是执行的指令数量多于原始方法，并行度的提升并没有带来性能的提升。
4	总结
本实验对 cache 优化和超标量优化进行了算法的设计与实验的分析，证实了其优化算法的有效性，讨论了为什么这种算法可以对程序的性能进行优化。本实验的代码开源在https://gitee.com/
YifanQi/bingxing。
